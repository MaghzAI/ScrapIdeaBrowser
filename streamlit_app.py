#!/usr/bin/env python3
"""
Ø¨Ø¯ÙŠÙ„ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„ Flutter
ÙŠØ³ØªØ®Ø¯Ù… Streamlit Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Flet Ù„Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Render
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
from pathlib import Path
import pandas as pd
from datetime import datetime
import time
import browser_cookie3
import json

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Web Scraper Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¸Ù‡Ø±
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        border: none;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        background-color: #45a049;
        transform: translateY(-2px);
    }
    .status-box {
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid;
    }
    .success { border-left-color: #4CAF50; background-color: #f8fff8; }
    .error { border-left-color: #f44336; background-color: #fff8f8; }
    .warning { border-left-color: #ff9800; background-color: #fffef8; }
    .info { border-left-color: #2196F3; background-color: #f8fbff; }
</style>
""", unsafe_allow_html=True)

class StreamlitScraperApp:
    def __init__(self):
        self.session = requests.Session()
        self.scraped_urls = set()
        self.failed_urls = set()

    def main(self):
        """Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        st.markdown("""
        <div class="main-header">
            <h1>ğŸš€ Web Scraper Pro</h1>
            <p>Ø£Ø¯Ø§Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ÙƒØ´Ø· ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨</p>
        </div>
        """, unsafe_allow_html=True)

        # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
        with st.sidebar:
            st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
            self.show_settings()

        # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        tab1, tab2, tab3 = st.tabs(["ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "ğŸ“‹ Ø§Ù„Ø³Ø¬Ù„"])

        with tab1:
            self.show_main_interface()

        with tab2:
            self.show_statistics()

        with tab3:
            self.show_logs()

    def show_settings(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
        st.subheader("ğŸŒ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„")

        # Ø§Ù„Ù†Ø·Ø§Ù‚
        self.domain = st.text_input(
            "Ø§Ù„Ù†Ø·Ø§Ù‚ (Domain)",
            value="ideabrowser.com",
            help="Ù…Ø«Ø§Ù„: github.com, stackoverflow.com"
        )

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØµÙØ­
        self.browser = st.selectbox(
            "Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²",
            ["auto", "chrome", "firefox", "edge", "safari", "opera"],
            index=1,
            help="Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…ÙˆØµÙ‰ Ø¨Ù‡"
        )

        # Ø²Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        if st.button("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Cookies ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„", type="primary"):
            self.extract_cookies_and_test()

    def show_main_interface(self):
        """Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        st.subheader("ğŸ” Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØ´Ø·")

        col1, col2 = st.columns(2)

        with col1:
            self.main_url = st.text_input(
                "Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
                value="https://www.ideabrowser.com/idea-of-the-day",
                help="Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"
            )

            self.element_id = st.text_input(
                "Ù…Ø¹Ø±Ù Ø§Ù„Ø¹Ù†ØµØ± (ID/Class)",
                value="main-wrapper",
                help="Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"
            )

        with col2:
            self.depth = st.selectbox(
                "Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ´Ø·",
                ["1", "2", "3", "unlimited"],
                index=1,
                help="Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ù„ÙƒØ´Ø·"
            )

            self.export_format = st.selectbox(
                "ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØµØ¯ÙŠØ±",
                ["markdown", "pdf", "both"],
                index=2,
                help="ØªÙ†Ø³ÙŠÙ‚ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"
            )

        # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
        self.save_folder = st.text_input(
            "Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸",
            value=str(Path.home() / "ScrapedContent"),
            help="Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª"
        )

        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ÙƒØ´Ø·", type="primary", use_container_width=True):
                self.start_scraping()

        with col2:
            if st.button("â¸ï¸ Ø¥ÙŠÙ‚Ø§Ù", use_container_width=True):
                st.warning("âš ï¸ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© Ø³ØªÙƒÙˆÙ† Ù…ØªØ§Ø­Ø© Ù‚Ø±ÙŠØ¨Ø§Ù‹")

        with col3:
            if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†", use_container_width=True):
                st.success("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")

    def show_statistics(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ´Ø·")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø©", len(self.scraped_urls), "âœ…")

        with col2:
            st.metric("Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©", len(self.failed_urls), "âŒ")

        with col3:
            st.metric("Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©", len(self.scraped_urls) + len(self.failed_urls), "ğŸ”—")

        with col4:
            success_rate = 0
            if len(self.scraped_urls) + len(self.failed_urls) > 0:
                success_rate = (len(self.scraped_urls) / (len(self.scraped_urls) + len(self.failed_urls))) * 100
            st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­", ".1f")

        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        if self.scraped_urls:
            st.subheader("âœ… Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø© Ø¨Ù†Ø¬Ø§Ø­")
            for url in list(self.scraped_urls)[:10]:  # Ø£Ø¸Ù‡Ø± Ø£ÙˆÙ„ 10 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
                st.write(f"â€¢ {url}")

        if self.failed_urls:
            st.subheader("âŒ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©")
            for url in list(self.failed_urls)[:5]:  # Ø£Ø¸Ù‡Ø± Ø£ÙˆÙ„ 5 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
                st.write(f"â€¢ {url}")

    def show_logs(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„"""
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª")

        if not hasattr(st.session_state, 'logs'):
            st.session_state.logs = []

        # Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„
        for log_entry in reversed(st.session_state.logs[-20:]):  # Ø£Ø¸Ù‡Ø± Ø¢Ø®Ø± 20 Ø¥Ø¯Ø®Ø§Ù„
            timestamp, message, log_type = log_entry
            if log_type == "success":
                st.success(f"[{timestamp}] {message}")
            elif log_type == "error":
                st.error(f"[{timestamp}] {message}")
            elif log_type == "warning":
                st.warning(f"[{timestamp}] {message}")
            else:
                st.info(f"[{timestamp}] {message}")

    def log(self, message, log_type="info"):
        """Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ø³Ø¬Ù„"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        if not hasattr(st.session_state, 'logs'):
            st.session_state.logs = []
        st.session_state.logs.append((timestamp, message, log_type))

    def extract_cookies_and_test(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„"""
        with st.spinner("ğŸ”„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²..."):
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                if self.browser == "auto":
                    browsers = ["chrome", "firefox", "edge", "safari", "opera"]
                    cookies_found = False
                    for browser_name in browsers:
                        try:
                            if browser_name == "chrome":
                                cookies = browser_cookie3.chrome(domain_name=self.domain)
                            elif browser_name == "firefox":
                                cookies = browser_cookie3.firefox(domain_name=self.domain)
                            elif browser_name == "edge":
                                cookies = browser_cookie3.edge(domain_name=self.domain)
                            elif browser_name == "safari":
                                cookies = browser_cookie3.safari(domain_name=self.domain)
                            elif browser_name == "opera":
                                cookies = browser_cookie3.opera(domain_name=self.domain)

                            cookies_list = list(cookies)
                            if cookies_list:
                                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù„Ù„Ø¬Ù„Ø³Ø©
                                for cookie in cookies_list:
                                    self.session.cookies.set(
                                        cookie.name,
                                        cookie.value,
                                        domain=cookie.domain,
                                        path=cookie.path
                                    )
                                self.log(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(cookies_list)} ÙƒÙˆÙƒÙŠ ÙÙŠ {browser_name}", "success")
                                cookies_found = True
                                break
                        except:
                            continue

                    if not cookies_found:
                        self.log("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙˆÙƒÙŠØ²", "error")
                        return
                else:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ù…ØªØµÙØ­ Ù…Ø­Ø¯Ø¯
                    try:
                        if self.browser == "chrome":
                            cookies = browser_cookie3.chrome(domain_name=self.domain)
                        elif self.browser == "firefox":
                            cookies = browser_cookie3.firefox(domain_name=self.domain)
                        elif self.browser == "edge":
                            cookies = browser_cookie3.edge(domain_name=self.domain)
                        elif self.browser == "safari":
                            cookies = browser_cookie3.safari(domain_name=self.domain)
                        elif self.browser == "opera":
                            cookies = browser_cookie3.opera(domain_name=self.domain)

                        cookies_list = list(cookies)
                        for cookie in cookies_list:
                            self.session.cookies.set(
                                cookie.name,
                                cookie.value,
                                domain=cookie.domain,
                                path=cookie.path
                            )
                        self.log(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(cookies_list)} ÙƒÙˆÙƒÙŠ", "success")
                    except Exception as e:
                        self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {str(e)}", "error")
                        return

                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                test_url = f"https://{self.domain}"
                try:
                    response = self.session.get(test_url, timeout=10)
                    if response.status_code == 200:
                        self.log("âœ… Ù†Ø¬Ø­ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„! Ø¬Ø§Ù‡Ø² Ù„Ù„ÙƒØ´Ø·", "success")
                        st.success("ğŸ‰ Ù†Ø¬Ø­ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„! Ø¬Ø§Ù‡Ø² Ù„Ù„ÙƒØ´Ø·")
                    else:
                        self.log(f"âš ï¸ Ø±Ù…Ø² Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.status_code}", "warning")
                        st.warning(f"âš ï¸ Ø±Ù…Ø² Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.status_code}")
                except Exception as e:
                    self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {str(e)}", "error")
                    st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {str(e)}")

            except Exception as e:
                self.log(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {str(e)}", "error")
                st.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {str(e)}")

    def start_scraping(self):
        """Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ø·"""
        if not self.main_url or not self.element_id:
            st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
            return

        with st.spinner("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ø·..."):
            self.log("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ÙƒØ´Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", "success")

            try:
                # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                content = self.get_page_content(self.main_url)
                if not content:
                    st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")
                    return

                # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                success = self.save_content(self.main_url, content, self.save_folder)
                if success:
                    self.scraped_urls.add(self.main_url)
                    self.log("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù†Ø¬Ø§Ø­", "success")
                    st.success("ğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ÙƒØ´Ø· Ø¨Ù†Ø¬Ø§Ø­!")
                else:
                    self.failed_urls.add(self.main_url)
                    self.log("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰", "error")
                    st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")

            except Exception as e:
                self.log(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ø·: {str(e)}", "error")
                st.error(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ø·: {str(e)}")

    def get_page_content(self, url):
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ar,en-US,en;q=0.5',
            }

            response = self.session.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            return response.text

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ {url}: {str(e)}", "error")
            return None

    def save_content(self, url, content, folder):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            os.makedirs(folder, exist_ok=True)

            # Ø­ÙØ¸ ÙƒÙ€ Markdown
            if self.export_format in ["markdown", "both"]:
                success_md = self.save_as_markdown(url, content, folder)
            else:
                success_md = True

            # Ø­ÙØ¸ ÙƒÙ€ PDF (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
            if self.export_format in ["pdf", "both"]:
                success_pdf = self.save_as_pdf(url, content, folder)
            else:
                success_pdf = True

            return success_md and success_pdf

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {str(e)}", "error")
            return False

    def save_as_markdown(self, url, content, folder):
        """Ø­ÙØ¸ ÙƒÙ€ Markdown"""
        try:
            from urllib.parse import urlparse
            import html2text

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(content, 'html.parser')

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            parsed_url = urlparse(url)
            if url == self.main_url:
                filename = "main_page.md"
            else:
                filename = "page.md"

            filepath = os.path.join(folder, filename)

            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Markdown
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            markdown_content = h.handle(str(soup))

            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            file_size = os.path.getsize(filepath) / 1024
            self.log(f"ğŸ“ ØªÙ… Ø­ÙØ¸ Markdown: {filename} ({file_size:.1f} KB)", "success")
            return True

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Markdown: {str(e)}", "error")
            return False

    def save_as_pdf(self, url, content, folder):
        """Ø­ÙØ¸ ÙƒÙ€ PDF"""
        try:
            from weasyprint import HTML
            from urllib.parse import urlparse

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(content, 'html.parser')

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            parsed_url = urlparse(url)
            if url == self.main_url:
                filename = "main_page.pdf"
            else:
                filename = "page.pdf"

            filepath = os.path.join(folder, filename)

            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ PDF
            html_doc = HTML(string=str(soup))
            html_doc.write_pdf(filepath)

            file_size = os.path.getsize(filepath) / 1024
            self.log(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ PDF: {filename} ({file_size:.1f} KB)", "success")
            return True

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ PDF: {str(e)}", "error")
            return False

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == "__main__":
    app = StreamlitScraperApp()
    app.main()
