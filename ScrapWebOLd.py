#!/usr/bin/env python3
"""
Ø¨Ø¯ÙŠÙ„ Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„ Flutter
ÙŠØ³ØªØ®Ø¯Ù… Streamlit Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Flet Ù„Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Render
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
from pathlib import Path
import pandas as pd
from datetime import datetime
import time
import browser_cookie3
import json
import zipfile
import smtplib
import ssl
from email.message import EmailMessage

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Web Scraper Pro",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¸Ù‡Ø±
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        border: none;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        background-color: #45a049;
        transform: translateY(-2px);
    }
    .status-box {
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 4px solid;
    }
    .success { border-left-color: #4CAF50; background-color: #f8fff8; }
    .error { border-left-color: #f44336; background-color: #fff8f8; }
    .warning { border-left-color: #ff9800; background-color: #fffef8; }
    .info { border-left-color: #2196F3; background-color: #f8fbff; }
</style>
""", unsafe_allow_html=True)

class StreamlitScraperApp:
    def __init__(self):
        self.session = requests.Session()
        self.scraped_urls = set()
        self.failed_urls = set()
        self.archives = []  # Ù„Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙØ§Øª

    def main(self):
        """Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        st.markdown("""
        <div class="main-header">
            <h1>ğŸš€ Web Scraper Pro</h1>
            <p>Ø£Ø¯Ø§Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ÙƒØ´Ø· ØµÙØ­Ø§Øª Ø§Ù„ÙˆÙŠØ¨</p>
        </div>
        """, unsafe_allow_html=True)

        # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
        with st.sidebar:
            st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
            self.show_settings()

        # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        tab1, tab2, tab3 = st.tabs(["ğŸ  Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "ğŸ“‹ Ø§Ù„Ø³Ø¬Ù„"])

        with tab1:
            self.show_main_interface()
            self.show_archives_table()

        with tab2:
            self.show_statistics()

        with tab3:
            self.show_logs()

    def show_settings(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
        st.subheader("ğŸŒ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„")

        # Ø§Ù„Ù†Ø·Ø§Ù‚
        self.domain = st.text_input(
            "Ø§Ù„Ù†Ø·Ø§Ù‚ (Domain)",
            value="ideabrowser.com",
            help="Ù…Ø«Ø§Ù„: github.com, stackoverflow.com"
        )

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØªØµÙØ­
        self.browser = st.selectbox(
            "Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²",
            ["auto", "chrome", "firefox", "edge", "safari", "opera"],
            index=1,
            help="Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…ÙˆØµÙ‰ Ø¨Ù‡"
        )

        # Ø²Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        if st.button("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Cookies ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„", type="primary"):
            self.extract_cookies_and_test()

    def show_main_interface(self):
        """Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        st.subheader("ğŸ” Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØ´Ø·")

        col1, col2 = st.columns(2)

        with col1:
            self.main_url = st.text_input(
                "Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
                value="https://www.ideabrowser.com/idea-of-the-day",
                help="Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"
            )

            self.element_id = st.text_input(
                "Ù…Ø¹Ø±Ù Ø§Ù„Ø¹Ù†ØµØ± (ID/Class)",
                value="main-wrapper",
                help="Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"
            )

        with col2:
            self.depth = st.selectbox(
                "Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ´Ø·",
                ["1", "2", "3", "unlimited"],
                index=1,
                help="Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ù„Ù„ÙƒØ´Ø·"
            )

            self.export_format = st.selectbox(
                "ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØµØ¯ÙŠØ±",
                ["markdown", "pdf", "both"],
                index=2,
                help="ØªÙ†Ø³ÙŠÙ‚ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"
            )

        # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸
        self.save_folder = st.text_input(
            "Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­ÙØ¸",
            value=str(Path.home() / "ScrapedContent"),
            help="Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª"
        )

        # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ÙƒØ´Ø·", type="primary", use_container_width=True):
                self.start_scraping()

        with col2:
            if st.button("â¸ï¸ Ø¥ÙŠÙ‚Ø§Ù", use_container_width=True):
                st.warning("âš ï¸ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© Ø³ØªÙƒÙˆÙ† Ù…ØªØ§Ø­Ø© Ù‚Ø±ÙŠØ¨Ø§Ù‹")

        with col3:
            if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†", use_container_width=True):
                st.success("âœ… ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")

    def show_statistics(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ´Ø·")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø©", len(self.scraped_urls), "âœ…")

        with col2:
            st.metric("Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©", len(self.failed_urls), "âŒ")

        with col3:
            st.metric("Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©", len(self.scraped_urls) + len(self.failed_urls), "ğŸ”—")

        with col4:
            success_rate = 0
            if len(self.scraped_urls) + len(self.failed_urls) > 0:
                success_rate = (len(self.scraped_urls) / (len(self.scraped_urls) + len(self.failed_urls))) * 100
            st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­", ".1f")

        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        if self.scraped_urls:
            st.subheader("âœ… Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙƒØ´ÙˆØ·Ø© Ø¨Ù†Ø¬Ø§Ø­")
            for url in list(self.scraped_urls)[:10]:  # Ø£Ø¸Ù‡Ø± Ø£ÙˆÙ„ 10 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
                st.write(f"â€¢ {url}")

        if self.failed_urls:
            st.subheader("âŒ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©")
            for url in list(self.failed_urls)[:5]:  # Ø£Ø¸Ù‡Ø± Ø£ÙˆÙ„ 5 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
                st.write(f"â€¢ {url}")

    def show_logs(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„"""
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª")

        if not hasattr(st.session_state, 'logs'):
            st.session_state.logs = []

        # Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„
        for log_entry in reversed(st.session_state.logs[-20:]):  # Ø£Ø¸Ù‡Ø± Ø¢Ø®Ø± 20 Ø¥Ø¯Ø®Ø§Ù„
            timestamp, message, log_type = log_entry
            if log_type == "success":
                st.success(f"[{timestamp}] {message}")
            elif log_type == "error":
                st.error(f"[{timestamp}] {message}")
            elif log_type == "warning":
                st.warning(f"[{timestamp}] {message}")
            else:
                st.info(f"[{timestamp}] {message}")

    def log(self, message, log_type="info"):
        """Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ø³Ø¬Ù„"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        if not hasattr(st.session_state, 'logs'):
            st.session_state.logs = []
        st.session_state.logs.append((timestamp, message, log_type))

    def extract_cookies_and_test(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„"""
        with st.spinner("ğŸ”„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²..."):
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
                if self.browser == "auto":
                    browsers = ["chrome", "firefox", "edge", "safari", "opera"]
                    cookies_found = False
                    for browser_name in browsers:
                        try:
                            if browser_name == "chrome":
                                cookies = browser_cookie3.chrome(domain_name=self.domain)
                            elif browser_name == "firefox":
                                cookies = browser_cookie3.firefox(domain_name=self.domain)
                            elif browser_name == "edge":
                                cookies = browser_cookie3.edge(domain_name=self.domain)
                            elif browser_name == "safari":
                                cookies = browser_cookie3.safari(domain_name=self.domain)
                            elif browser_name == "opera":
                                cookies = browser_cookie3.opera(domain_name=self.domain)

                            cookies_list = list(cookies)
                            if cookies_list:
                                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù„Ù„Ø¬Ù„Ø³Ø©
                                for cookie in cookies_list:
                                    self.session.cookies.set(
                                        cookie.name,
                                        cookie.value,
                                        domain=cookie.domain,
                                        path=cookie.path
                                    )
                                self.log(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(cookies_list)} ÙƒÙˆÙƒÙŠ ÙÙŠ {browser_name}", "success")
                                cookies_found = True
                                break
                        except:
                            continue

                    if not cookies_found:
                        self.log("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙˆÙƒÙŠØ²", "error")
                        return
                else:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ù…ØªØµÙØ­ Ù…Ø­Ø¯Ø¯
                    try:
                        if self.browser == "chrome":
                            cookies = browser_cookie3.chrome(domain_name=self.domain)
                        elif self.browser == "firefox":
                            cookies = browser_cookie3.firefox(domain_name=self.domain)
                        elif self.browser == "edge":
                            cookies = browser_cookie3.edge(domain_name=self.domain)
                        elif self.browser == "safari":
                            cookies = browser_cookie3.safari(domain_name=self.domain)
                        elif self.browser == "opera":
                            cookies = browser_cookie3.opera(domain_name=self.domain)

                        cookies_list = list(cookies)
                        for cookie in cookies_list:
                            self.session.cookies.set(
                                cookie.name,
                                cookie.value,
                                domain=cookie.domain,
                                path=cookie.path
                            )
                        self.log(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(cookies_list)} ÙƒÙˆÙƒÙŠ", "success")
                    except Exception as e:
                        self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {str(e)}", "error")
                        return

                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                test_url = f"https://{self.domain}"
                try:
                    response = self.session.get(test_url, timeout=10)
                    if response.status_code == 200:
                        self.log("âœ… Ù†Ø¬Ø­ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„! Ø¬Ø§Ù‡Ø² Ù„Ù„ÙƒØ´Ø·", "success")
                        st.success("ğŸ‰ Ù†Ø¬Ø­ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„! Ø¬Ø§Ù‡Ø² Ù„Ù„ÙƒØ´Ø·")
                    else:
                        self.log(f"âš ï¸ Ø±Ù…Ø² Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.status_code}", "warning")
                        st.warning(f"âš ï¸ Ø±Ù…Ø² Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.status_code}")
                except Exception as e:
                    self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {str(e)}", "error")
                    st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {str(e)}")

            except Exception as e:
                self.log(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {str(e)}", "error")
                st.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {str(e)}")

    def start_scraping(self):
        """Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ø· Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ÙƒØ´Ø· Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"""
        if not self.main_url or not self.element_id:
            st.error("âŒ ÙŠØ±Ø¬Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©")
            return

        with st.spinner("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ´Ø·..."):
            self.log("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ÙƒØ´Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", "success")
            try:
                # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…Ù‚
                urls_queue = [(self.main_url, 0)]  # (url, depth)
                processed_count = 0
                max_depth = self.depth
                visited = set()
                while urls_queue:
                    current_url, depth = urls_queue.pop(0)
                    if max_depth != "unlimited" and int(depth) >= int(max_depth):
                        continue
                    if current_url in visited:
                        continue
                    processed_count += 1
                    visited.add(current_url)
                    content = self.get_page_content(current_url)
                    if not content:
                        self.failed_urls.add(current_url)
                        continue
                    success = self.save_content(current_url, content, self.save_folder)
                    if success:
                        self.scraped_urls.add(current_url)
                        self.log(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ [{processed_count}]: {current_url}", "success")
                    else:
                        self.failed_urls.add(current_url)
                        self.log(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {current_url}", "error")
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø· Ø£Ùˆ ÙƒÙ„ ØµÙØ­Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù‚
                    if depth == 0:
                        new_links = self.extract_links(content, current_url, self.element_id)
                        for link in new_links:
                            if link not in visited:
                                urls_queue.append((link, depth + 1))
                # Ø¶ØºØ· Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„ÙƒØ´Ø·
                zip_path = self.zip_folder(self.save_folder)
                st.success(f"ğŸ—œï¸ ØªÙ… Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª: {zip_path}")
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ· Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
                self.send_email_with_attachment(zip_path)
                # Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ù„Ù„Ø¬Ø¯ÙˆÙ„
                archive_info = {
                    "project": os.path.basename(self.save_folder),
                    "zip": zip_path,
                    "date": datetime.now().strftime('%Y-%m-%d %H:%M'),
                }
                self.archives.append(archive_info)
                st.success("ğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ÙƒØ´Ø· Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
            except Exception as e:
                self.log(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ø·: {str(e)}", "error")
                st.error(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ø·: {str(e)}")

    def extract_links(self, html, base_url, element_id):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø­Ø¯Ø¯
        element = soup.find(id=element_id) or soup.find(class_=element_id)
        if not element:
            try:
                element = soup.select_one(element_id)
            except Exception:
                pass
        if not element:
            return links
        for link_tag in element.find_all('a', href=True):
            href = link_tag.get('href', '').strip()
            if not href or href.startswith('#') or href.startswith('javascript:') or href.startswith('mailto:'):
                continue
            full_url = requests.compat.urljoin(base_url, href)
            if requests.utils.urlparse(full_url).netloc == requests.utils.urlparse(base_url).netloc:
                clean_url = full_url.split('#')[0].rstrip('/')
                if clean_url not in links and clean_url != base_url:
                    links.append(clean_url)
        return list(set(links))

    def zip_folder(self, folder_path):
        """Ø¶ØºØ· Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø¥Ù„Ù‰ Ù…Ù„Ù ZIP"""
        zip_path = folder_path.rstrip(os.sep) + ".zip"
        base, ext = os.path.splitext(zip_path)
        i = 1
        while os.path.exists(zip_path):
            zip_path = f"{base}_{i}{ext}"
            i += 1
        with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, os.path.dirname(folder_path))
                    zf.write(full_path, arcname=rel_path)
        return zip_path

    def send_email_with_attachment(self, file_path):
        """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ø¹ Ø§Ù„Ù…Ø±ÙÙ‚ Ø¥Ù„Ù‰ dsyemen2020@gmail.com"""
        try:
            SMTP_HOST = os.environ.get("SMTP_HOST", "smtp.gmail.com")
            SMTP_PORT = int(os.environ.get("SMTP_PORT", 587))
            USER = os.environ.get("SMTP_USER", "")
            PASSWORD = os.environ.get("SMTP_PASS", "")
            TO = "dsyemen2020@gmail.com"
            if not USER or not PASSWORD:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª SMTP (SMTP_USER/SMTP_PASS)")
                return
            msg = EmailMessage()
            msg["Subject"] = f"Ù†ØªØ§Ø¦Ø¬ ÙƒØ´Ø· Ø§Ù„ÙˆÙŠØ¨ - {os.path.basename(file_path)}"
            msg["From"] = USER
            msg["To"] = TO
            msg.set_content(f"""
Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡ØŒ

ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© ÙƒØ´Ø· Ø§Ù„ÙˆÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­.

ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:
â€¢ Ø§Ø³Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {os.path.basename(file_path).replace('.zip', '')}
â€¢ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: {os.path.basename(file_path)}
â€¢ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
â€¢ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {os.path.getsize(file_path) / (1024*1024):.2f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª

Ù…Ø¹ ØªØ­ÙŠØ§ØªØŒ
Web Scraper Pro
""")
            with open(file_path, "rb") as f:
                file_data = f.read()
            msg.add_attachment(file_data, maintype="application", subtype="zip", filename=os.path.basename(file_path))
            context = ssl.create_default_context()
            with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as smtp:
                smtp.starttls(context=context)
                smtp.login(USER, PASSWORD)
                smtp.send_message(msg)
            st.success(f"ğŸ“§ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ø¥Ù„Ù‰ {TO}")
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨Ø±ÙŠØ¯: {str(e)}")

    def show_archives_table(self):
        """Ø¹Ø±Ø¶ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø±Ø´ÙŠÙØ§Øª Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        if self.archives:
            st.subheader("ğŸ“‚ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø±Ø´ÙŠÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©")
            df = pd.DataFrame(self.archives)
            def make_download_link(zip_path):
                if os.path.exists(zip_path):
                    return f'<a href="file://{zip_path}" download>ØªØ­Ù…ÙŠÙ„</a>'
                return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            df['ØªØ­Ù…ÙŠÙ„'] = df['zip'].apply(make_download_link)
            st.write(df.to_html(escape=False, index=False), unsafe_allow_html=True)

    def get_page_content(self, url):
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ar,en-US,en;q=0.5',
            }

            response = self.session.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            return response.text

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ {url}: {str(e)}", "error")
            return None

    def save_content(self, url, content, folder):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            os.makedirs(folder, exist_ok=True)

            # Ø­ÙØ¸ ÙƒÙ€ Markdown
            if self.export_format in ["markdown", "both"]:
                success_md = self.save_as_markdown(url, content, folder)
            else:
                success_md = True

            # Ø­ÙØ¸ ÙƒÙ€ PDF (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
            if self.export_format in ["pdf", "both"]:
                success_pdf = self.save_as_pdf(url, content, folder)
            else:
                success_pdf = True

            return success_md and success_pdf

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {str(e)}", "error")
            return False

    def save_as_markdown(self, url, content, folder):
        """Ø­ÙØ¸ ÙƒÙ€ Markdown"""
        try:
            from urllib.parse import urlparse
            import html2text

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(content, 'html.parser')

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            parsed_url = urlparse(url)
            if url == self.main_url:
                filename = "main_page.md"
            else:
                filename = "page.md"

            filepath = os.path.join(folder, filename)

            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Markdown
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            markdown_content = h.handle(str(soup))

            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            file_size = os.path.getsize(filepath) / 1024
            self.log(f"ğŸ“ ØªÙ… Ø­ÙØ¸ Markdown: {filename} ({file_size:.1f} KB)", "success")
            return True

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Markdown: {str(e)}", "error")
            return False

    def save_as_pdf(self, url, content, folder):
        """Ø­ÙØ¸ ÙƒÙ€ PDF"""
        try:
            from weasyprint import HTML
            from urllib.parse import urlparse

            # ØªØ­Ù„ÙŠÙ„ HTML
            soup = BeautifulSoup(content, 'html.parser')

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            parsed_url = urlparse(url)
            if url == self.main_url:
                filename = "main_page.pdf"
            else:
                filename = "page.pdf"

            filepath = os.path.join(folder, filename)

            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ PDF
            html_doc = HTML(string=str(soup))
            html_doc.write_pdf(filepath)

            file_size = os.path.getsize(filepath) / 1024
            self.log(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ PDF: {filename} ({file_size:.1f} KB)", "success")
            return True

        except Exception as e:
            self.log(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ PDF: {str(e)}", "error")
            return False

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
if __name__ == "__main__":
    app = StreamlitScraperApp()
    app.main()
